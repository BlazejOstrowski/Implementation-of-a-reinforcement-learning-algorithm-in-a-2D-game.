{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D\n",
    "\n",
    "\n",
    "def build_model(height, width, actions):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Convolution2D(\n",
    "            64,\n",
    "            (4, 4),\n",
    "            strides=(2, 2),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(3, height, width),\n",
    "            padding=\"same\",\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Convolution2D(32, (2, 2), strides=(1, 1), activation=\"relu\", padding=\"same\")\n",
    "    )\n",
    "    model.add(Convolution2D(32, (2, 2), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(actions, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"screen_width\": 640,\n",
    "    \"screen_height\": 480,\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"observation_shape\": (10, 10),\n",
    "        \"vehicles_count\": 10,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20],\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"sorted\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "NB_STEPS = 1000000  # Amount of steps to be used to train the model\n",
    "\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.1,\n",
    "        value_test=0.2,\n",
    "        nb_steps=NB_STEPS,\n",
    "    )\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        memory=memory,\n",
    "        policy=policy,\n",
    "        enable_dueling_network=True,\n",
    "        dueling_type=\"avg\",\n",
    "        nb_actions=actions,\n",
    "        nb_steps_warmup=NB_STEPS / 100,\n",
    "    )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:112: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001B[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4/1000000: episode: 1, duration: 0.661s, episode steps:   4, steps per second:   6, episode reward:  2.566, mean reward:  0.642 [ 0.000,  0.867], mean action: 0.750 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     23/1000000: episode: 2, duration: 1.123s, episode steps:  19, steps per second:  17, episode reward: 16.643, mean reward:  0.876 [ 0.033,  0.967], mean action: 1.263 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     28/1000000: episode: 3, duration: 0.316s, episode steps:   5, steps per second:  16, episode reward:  3.465, mean reward:  0.693 [ 0.067,  0.867], mean action: 1.600 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     35/1000000: episode: 4, duration: 0.442s, episode steps:   7, steps per second:  16, episode reward:  5.032, mean reward:  0.719 [ 0.133,  0.833], mean action: 0.714 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     39/1000000: episode: 5, duration: 0.254s, episode steps:   4, steps per second:  16, episode reward:  2.616, mean reward:  0.654 [ 0.000,  0.949], mean action: 1.250 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     49/1000000: episode: 6, duration: 0.588s, episode steps:  10, steps per second:  17, episode reward:  8.712, mean reward:  0.871 [ 0.067,  1.000], mean action: 1.700 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     53/1000000: episode: 7, duration: 0.255s, episode steps:   4, steps per second:  16, episode reward:  2.782, mean reward:  0.695 [ 0.067,  0.982], mean action: 2.000 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     60/1000000: episode: 8, duration: 0.413s, episode steps:   7, steps per second:  17, episode reward:  5.315, mean reward:  0.759 [ 0.033,  0.982], mean action: 1.286 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     67/1000000: episode: 9, duration: 0.425s, episode steps:   7, steps per second:  16, episode reward:  5.446, mean reward:  0.778 [ 0.033,  0.982], mean action: 1.714 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     75/1000000: episode: 10, duration: 0.608s, episode steps:   8, steps per second:  13, episode reward:  6.613, mean reward:  0.827 [ 0.033,  0.967], mean action: 2.000 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     80/1000000: episode: 11, duration: 0.322s, episode steps:   5, steps per second:  16, episode reward:  3.446, mean reward:  0.689 [ 0.000,  0.931], mean action: 1.200 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     89/1000000: episode: 12, duration: 0.650s, episode steps:   9, steps per second:  14, episode reward:  7.746, mean reward:  0.861 [ 0.033,  1.000], mean action: 1.667 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "     98/1000000: episode: 13, duration: 0.604s, episode steps:   9, steps per second:  15, episode reward:  7.046, mean reward:  0.783 [ 0.000,  0.933], mean action: 1.222 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    102/1000000: episode: 14, duration: 0.291s, episode steps:   4, steps per second:  14, episode reward:  2.499, mean reward:  0.625 [ 0.033,  0.833], mean action: 1.250 [1.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    112/1000000: episode: 15, duration: 0.665s, episode steps:  10, steps per second:  15, episode reward:  8.880, mean reward:  0.888 [ 0.033,  1.000], mean action: 2.100 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    115/1000000: episode: 16, duration: 0.202s, episode steps:   3, steps per second:  15, episode reward:  1.946, mean reward:  0.649 [ 0.067,  0.964], mean action: 2.333 [2.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    127/1000000: episode: 17, duration: 0.770s, episode steps:  12, steps per second:  16, episode reward: 10.445, mean reward:  0.870 [ 0.033,  1.000], mean action: 1.667 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    140/1000000: episode: 18, duration: 0.821s, episode steps:  13, steps per second:  16, episode reward: 10.712, mean reward:  0.824 [ 0.267,  0.933], mean action: 1.000 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    145/1000000: episode: 19, duration: 0.424s, episode steps:   5, steps per second:  12, episode reward:  3.499, mean reward:  0.700 [ 0.067,  0.867], mean action: 1.200 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    152/1000000: episode: 20, duration: 0.475s, episode steps:   7, steps per second:  15, episode reward:  6.279, mean reward:  0.897 [ 0.333,  1.000], mean action: 2.143 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    168/1000000: episode: 21, duration: 0.967s, episode steps:  16, steps per second:  17, episode reward: 12.913, mean reward:  0.807 [ 0.300,  0.967], mean action: 1.000 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    176/1000000: episode: 22, duration: 0.510s, episode steps:   8, steps per second:  16, episode reward:  6.045, mean reward:  0.756 [ 0.000,  0.966], mean action: 1.125 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    179/1000000: episode: 23, duration: 0.188s, episode steps:   3, steps per second:  16, episode reward:  2.047, mean reward:  0.682 [ 0.067,  0.998], mean action: 2.333 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    186/1000000: episode: 24, duration: 0.440s, episode steps:   7, steps per second:  16, episode reward:  5.846, mean reward:  0.835 [ 0.267,  0.933], mean action: 1.714 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    198/1000000: episode: 25, duration: 0.756s, episode steps:  12, steps per second:  16, episode reward: 10.979, mean reward:  0.915 [ 0.033,  1.000], mean action: 2.167 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    204/1000000: episode: 26, duration: 0.363s, episode steps:   6, steps per second:  17, episode reward:  4.651, mean reward:  0.775 [ 0.105,  0.966], mean action: 1.000 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    208/1000000: episode: 27, duration: 0.259s, episode steps:   4, steps per second:  15, episode reward:  2.780, mean reward:  0.695 [ 0.033,  0.964], mean action: 1.750 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    216/1000000: episode: 28, duration: 0.486s, episode steps:   8, steps per second:  16, episode reward:  6.679, mean reward:  0.835 [ 0.300,  0.933], mean action: 0.875 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    220/1000000: episode: 29, duration: 0.252s, episode steps:   4, steps per second:  16, episode reward:  2.531, mean reward:  0.633 [ 0.067,  0.833], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    227/1000000: episode: 30, duration: 0.426s, episode steps:   7, steps per second:  16, episode reward:  5.612, mean reward:  0.802 [ 0.266,  0.966], mean action: 1.429 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    235/1000000: episode: 31, duration: 0.531s, episode steps:   8, steps per second:  15, episode reward:  5.931, mean reward:  0.741 [ 0.067,  0.867], mean action: 1.250 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    238/1000000: episode: 32, duration: 0.184s, episode steps:   3, steps per second:  16, episode reward:  1.716, mean reward:  0.572 [ 0.000,  0.916], mean action: 2.333 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    242/1000000: episode: 33, duration: 0.249s, episode steps:   4, steps per second:  16, episode reward:  3.046, mean reward:  0.762 [ 0.067,  1.000], mean action: 2.000 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    249/1000000: episode: 34, duration: 0.460s, episode steps:   7, steps per second:  15, episode reward:  5.845, mean reward:  0.835 [ 0.067,  1.000], mean action: 1.571 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    252/1000000: episode: 35, duration: 0.205s, episode steps:   3, steps per second:  15, episode reward:  1.732, mean reward:  0.577 [ 0.033,  0.866], mean action: 0.667 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    255/1000000: episode: 36, duration: 0.228s, episode steps:   3, steps per second:  13, episode reward:  1.947, mean reward:  0.649 [ 0.033,  0.964], mean action: 2.333 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    258/1000000: episode: 37, duration: 0.218s, episode steps:   3, steps per second:  14, episode reward:  1.915, mean reward:  0.638 [ 0.067,  0.982], mean action: 2.333 [2.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    263/1000000: episode: 38, duration: 0.355s, episode steps:   5, steps per second:  14, episode reward:  3.813, mean reward:  0.763 [ 0.067,  0.999], mean action: 1.800 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    278/1000000: episode: 39, duration: 0.992s, episode steps:  15, steps per second:  15, episode reward: 13.411, mean reward:  0.894 [ 0.000,  1.000], mean action: 1.200 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    281/1000000: episode: 40, duration: 0.199s, episode steps:   3, steps per second:  15, episode reward:  1.882, mean reward:  0.627 [ 0.033,  0.982], mean action: 1.667 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    286/1000000: episode: 41, duration: 0.317s, episode steps:   5, steps per second:  16, episode reward:  3.612, mean reward:  0.722 [ 0.000,  0.964], mean action: 1.800 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    297/1000000: episode: 42, duration: 0.725s, episode steps:  11, steps per second:  15, episode reward:  9.153, mean reward:  0.832 [ 0.107,  0.933], mean action: 1.364 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    301/1000000: episode: 43, duration: 0.277s, episode steps:   4, steps per second:  14, episode reward:  2.880, mean reward:  0.720 [ 0.033,  0.998], mean action: 1.750 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    308/1000000: episode: 44, duration: 0.470s, episode steps:   7, steps per second:  15, episode reward:  5.279, mean reward:  0.754 [ 0.033,  0.966], mean action: 1.429 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    312/1000000: episode: 45, duration: 0.240s, episode steps:   4, steps per second:  17, episode reward:  2.682, mean reward:  0.671 [ 0.067,  0.949], mean action: 1.750 [1.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    317/1000000: episode: 46, duration: 0.302s, episode steps:   5, steps per second:  17, episode reward:  3.713, mean reward:  0.743 [ 0.033,  0.964], mean action: 1.400 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "    324/1000000: episode: 47, duration: 0.440s, episode steps:   7, steps per second:  16, episode reward:  5.879, mean reward:  0.840 [ 0.067,  1.000], mean action: 2.000 [0.000, 3.000],  loss: --, mean_q: --, mean_eps: --\n",
      "done, took 21.422 seconds\n",
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 2.499, steps: 4\n",
      "Episode 2: reward: 4.365, steps: 6\n",
      "Episode 3: reward: 7.833, steps: 10\n",
      "Episode 4: reward: 6.233, steps: 8\n",
      "Episode 5: reward: 2.633, steps: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [24], line 56\u001B[0m\n\u001B[0;32m     54\u001B[0m dqn\u001B[38;5;241m.\u001B[39mcompile(Adam(lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m))\n\u001B[0;32m     55\u001B[0m training \u001B[38;5;241m=\u001B[39m dqn\u001B[38;5;241m.\u001B[39mfit(env, nb_steps\u001B[38;5;241m=\u001B[39mNB_STEPS, visualize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mdqn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHOW_MANY_EPISODES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m plot_results(training)\n\u001B[0;32m     58\u001B[0m dqn\u001B[38;5;241m.\u001B[39msave_weights(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved_weights/1k-fast.h5f\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\core.py:348\u001B[0m, in \u001B[0;36mAgent.test\u001B[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(action_repetition):\n\u001B[0;32m    347\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_action_begin(action)\n\u001B[1;32m--> 348\u001B[0m     observation, r, d, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m     observation \u001B[38;5;241m=\u001B[39m deepcopy(observation)\n\u001B[0;32m    350\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:216\u001B[0m, in \u001B[0;36mAbstractEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpolicy_frequency\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_simulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_type\u001B[38;5;241m.\u001B[39mobserve()\n\u001B[0;32m    219\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reward(action)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:236\u001B[0m, in \u001B[0;36mAbstractEnv._simulate\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_type\u001B[38;5;241m.\u001B[39mact(action)\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroad\u001B[38;5;241m.\u001B[39mact()\n\u001B[1;32m--> 236\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msimulation_frequency\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;66;03m# Ignored if the rendering is done offscreen\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\road\\road.py:333\u001B[0m, in \u001B[0;36mRoad.step\u001B[1;34m(self, dt)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    328\u001B[0m \u001B[38;5;124;03mStep the dynamics of each entity on the road.\u001B[39;00m\n\u001B[0;32m    329\u001B[0m \n\u001B[0;32m    330\u001B[0m \u001B[38;5;124;03m:param dt: timestep [s]\u001B[39;00m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m vehicle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvehicles:\n\u001B[1;32m--> 333\u001B[0m     \u001B[43mvehicle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, vehicle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvehicles):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvehicles[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m:]:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001B[0m, in \u001B[0;36mIDMVehicle.step\u001B[1;34m(self, dt)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03mStep the simulation.\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m:param dt: timestep\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimer \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m dt\n\u001B[1;32m--> 124\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\vehicle\\kinematics.py:131\u001B[0m, in \u001B[0;36mVehicle.step\u001B[1;34m(self, dt)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheading \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msin(beta) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLENGTH \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m*\u001B[39m dt\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeed \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macceleration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m*\u001B[39m dt\n\u001B[1;32m--> 131\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_state_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\vehicle\\kinematics.py:146\u001B[0m, in \u001B[0;36mVehicle.on_state_update\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_state_update\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroad:\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlane_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_closest_lane_index\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheading\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlane \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroad\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mget_lane(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlane_index)\n\u001B[0;32m    148\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroad\u001B[38;5;241m.\u001B[39mrecord_history:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\road\\road.py:61\u001B[0m, in \u001B[0;36mRoadNetwork.get_closest_lane_index\u001B[1;34m(self, position, heading)\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _to, lanes \u001B[38;5;129;01min\u001B[39;00m to_dict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     60\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m _id, l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(lanes):\n\u001B[1;32m---> 61\u001B[0m             distances\u001B[38;5;241m.\u001B[39mappend(\u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistance_with_heading\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheading\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     62\u001B[0m             indexes\u001B[38;5;241m.\u001B[39mappend((_from, _to, _id))\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m indexes[\u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39margmin(distances))]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\road\\lane.py:125\u001B[0m, in \u001B[0;36mAbstractLane.distance_with_heading\u001B[1;34m(self, position, heading, heading_weight)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m heading \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistance(position)\n\u001B[1;32m--> 125\u001B[0m s, r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_coordinates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposition\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m angle \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_angle(heading, s))\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mabs\u001B[39m(r) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mmax\u001B[39m(s \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m \u001B[38;5;241m-\u001B[39m s, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m+\u001B[39m heading_weight\u001B[38;5;241m*\u001B[39mangle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\highway_env\\road\\lane.py:189\u001B[0m, in \u001B[0;36mStraightLane.local_coordinates\u001B[1;34m(self, position)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlocal_coordinates\u001B[39m(\u001B[38;5;28mself\u001B[39m, position: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]:\n\u001B[0;32m    188\u001B[0m     delta \u001B[38;5;241m=\u001B[39m position \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart\n\u001B[1;32m--> 189\u001B[0m     longitudinal \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdirection\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m     lateral \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(delta, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdirection_lateral)\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(longitudinal), \u001B[38;5;28mfloat\u001B[39m(lateral)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "from agent import build_agent, NB_STEPS\n",
    "from conf import config\n",
    "from model import build_model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "HOW_MANY_EPISODES = 10  # Amount of episodes for testing purposes\n",
    "# ACTIONS_ALL = {0: \"LANE_LEFT\", 1: \"IDLE\", 2: \"LANE_RIGHT\", 3: \"FASTER\", 4: \"SLOWER\"}\n",
    "\n",
    "\n",
    "def plot_results(data):\n",
    "    plt.plot(\n",
    "        data.history[\"nb_steps\"],\n",
    "        data.history[\"episode_reward\"],\n",
    "    )\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\")\n",
    "env.configure(config)\n",
    "actions = env.get_available_actions()\n",
    "# print(actions)\n",
    "env.reset()\n",
    "height, width = env.observation_space.shape\n",
    "\n",
    "# episodes = 3\n",
    "# for episode in range(1, episodes):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         # print(f\"{n_state=}\")\n",
    "# env.close()\n",
    "\n",
    "model = build_model(height, width, len(actions))\n",
    "dqn = build_agent(model, len(actions))\n",
    "\n",
    "load = input(\"Do You want to load a already trained model?(y/n): \").lower()\n",
    "if len(os.listdir(\"saved_weights/\")) == 0 and load == \"y\":\n",
    "    print(\"Directory with models is empty. Exiting.\")\n",
    "    exit()\n",
    "elif load == \"y\":\n",
    "    dqn.load_weights(\"saved_weights/1k-fast.h5f\")\n",
    "    scores = dqn.test(env, nb_episodes=HOW_MANY_EPISODES, visualize=True)\n",
    "    plot_results(scores)\n",
    "else:\n",
    "    dqn.compile(Adam(lr=1e-4))\n",
    "    training = dqn.fit(env, nb_steps=NB_STEPS, visualize=False, verbose=2)\n",
    "    scores = dqn.test(env, nb_episodes=HOW_MANY_EPISODES, visualize=True)\n",
    "    plot_results(training)\n",
    "    dqn.save_weights(\"saved_weights/1k-fast.h5f\")\n",
    "# print(np.mean(scores.history[\"episode_reward\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}